{"0": {
    "doc": "Speech Recognition",
    "title": "Speech Recognition",
    "content": "# Speech Recognition `TODO` | Metric | Description |--------------------------------------+---------------------------------------------------------------| WER | Word Error Rate | Utterance False Positive Rate (uFPR) | Ratio of cases where non speech utterances were transcribed. | Utterance False Negative Rate (uFNR) | Ratio of cases where utterances where transcribed as silence. | ## Data schema `TODO`: Note about the `` situation in transcription. ## Usage ### Command Line Use the sub-command `asr` like shown below: ```shell eevee asr ./data/tagged.transcriptions.csv ./data/predicted.transcriptions.csv ``` ``` Value Support Metric WER 0.571429 6 Utterance FPR 0.500000 2 Utterance FNR 0.250000 4 ``` ### Python module `TODO` ",
    "url": "/eevee/metrics/asr.html",
    "relUrl": "/metrics/asr.html"
  },"1": {
    "doc": "Data Structures",
    "title": "Data Structures",
    "content": "# Data Structures Eevee works with CSV label dataframes with items as per [these definitions](https://github.com/skit-ai/dataframes/blob/master/protos/labels.proto). Since label dataframes have `id` for referring back to the data, we just focus on labels in this tool. These labels can be true labels of various kind or coming from predictions of different models. This page documents a few general notes about the label representation. Specific details are in the pages for different kinds of metrics [here](./metrics). ## Serialization Each row in the label dataframe CSV is of one of the types defined [here](https://github.com/skit-ai/dataframes/blob/master/protos/labels.proto). In cases where the field type are not primitives, we serialize items in JSON. In Python, this looks like the following: ```python import pandas as pd # Assuming each item in `items` is a list of entities rows = [{\"id\": i, \"entities\": json.dumps(it)} for i, it in enumerate(items)] pd.DataFrame(rows).to_csv(\"./predictions.csv\", index=False) ``` The following is how correctly serialized structure looks like in a labels CSV: ``` \"[[{\"\"am_score\"\": -278.4794, \"\"confidence\"\": 0.9739978, \"\"lm_score\"\": 13.827044, \"\"transcript\"\": \"\"no\"\"}]]\" ``` If you skip JSON dumping, tools like pandas might still serialize like following: ``` \"[[{'am_score': -278.4794, 'confidence': 0.9739978, 'lm_score': 13.827044, 'transcript': 'no'}]]\" ``` But this won't be read back in `eevee` and you will get a `JSONDecodeError` ``` JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 4 (char 3) ``` ",
    "url": "/eevee/data-structures.html",
    "relUrl": "/data-structures.html"
  },"2": {
    "doc": "Metrics",
    "title": "Metrics",
    "content": " ",
    "url": "/eevee/metrics/",
    "relUrl": "/metrics/"
  },"3": {
    "doc": "Home",
    "title": "Home",
    "content": "# Eevee ![](https://img.shields.io/github/v/tag/skit-ai/eevee.svg?style=flat-square) ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/skit-ai/eevee/CI?style=flat-square) `eevee` is a set of standard evaluation utilities for problems that we work on at [Skit](https://skit.ai). You can use `eevee` both as a python module or as a CLI tool. It works on data files with label structures from [dataframes](https://github.com/skit-ai/dataframes) that has standard datatype definitions. See `./data` directory for example files. ## Installation For now, you have to install eevee using Github release URLs. The current version can be installed by using the following: ```bash pip install https://github.com/skit-ai/eevee/releases/download/1.0.0/eevee-1.0.0-py3-none-any.whl ``` ## Usage Once installed, the most common usage pattern involves passing a reference and predicted label dataframes and get report either for human viewing, or get a json for further machine consumption. Here is how you use it for intents: ```bash eevee intent ./tagged.intent.csv ./predicted.intent.csv ``` Similarly, for WER report you can do this: ```bash eevee asr ./data/tagged.transcriptions.csv ./data/predicted.transcriptions.csv ``` ``` Value Support Metric WER 0.571429 6 Utterance FPR 0.500000 2 Utterance FNR 0.250000 4 ``` There are a few advanced unexposed metrics related to ASR. Since they are still work in progress, we have kept a few dependencies from there as _extras_. If you need those, you should install the package in development mode and do `poetry install -E asr`. Then follow the scripts in `./scripts`. ",
    "url": "/eevee/",
    "relUrl": "/"
  },"4": {
    "doc": "Intents",
    "title": "Intents",
    "content": "# Intents Eevee let's you calculate all the important _turn level metrics_ (precision, recall, f1) for intents. We tag these data points using tog, an internal tool ## Data Schema We expect the csv(s) to have `id` and `intent` columns. They will be inner-joined on `id`. `id` is expected from the user to be unique. `intent` column should have values whch are of `str` type. ## Usage ### Command Line Call the sub-command `intent` like shown below: ```shell eevee intent ./true-labels.csv ./pred-labels.csv ``` that takes up the csv's merges them on `id` column, to perform [sklearn's classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) on the intents. there is another feature, `alias`-ing ``` eevee intent ./true-labels.csv ./pred-labels.csv --alias-yaml=data/alias.yaml ``` This helps with aliasing/grouping intents as their respective group: In the sample file `alias.yaml` under `data` directory, we have intents (from the `true-labels.csv` and `pred-labels.csv`) grouped under: * smalltalk * oos (out-of-scope) but you could name or group the intents according to how you wish. The remaining intents which are not part of the groups are grouped as `in_scope` by default. This returns the `weighted_average` of [sklearn's precision_recall_fscore_support](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) Further granular analysis on aliasing/grouping is also possible. where each group has its own [sklearn's classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) using this: ``` eevee intent ./true-labels.csv ./pred-labels.csv --alias-yaml=data/alias.yaml --breakdown ``` ## JSON support All the above mentioned commands use cases, have additional `--json` flag which will be given out in stdout and can be parsed using tools like `jq`. ### Python module ```python >>> import pandas as pd >>> from eevee.metrics.classification import intent_report :: stanza not found >>> >>> true_df = pd.read_csv(\"data/reddoorz.tagged_data.csv\") >>> pred_df = pd.read_csv(\"data/reddoorz.labels.csv\") >>> >>> all_intents_classification_report = intent_report(true_df, pred_df) >>> print(all_intents_classification_report) precision recall f1-score support _cancel_ 1.00 1.00 1.00 100 _confirm_ 1.00 1.00 1.00 161 _greeting_ 1.00 1.00 1.00 134 _oos_ 1.00 1.00 1.00 97 ... refund_status 1.00 1.00 1.00 8 request_agent 1.00 1.00 1.00 28 short_utterance 1.00 1.00 1.00 23 accuracy 1.00 1035 macro avg 1.00 1.00 1.00 1035 weighted avg 1.00 1.00 1.00 1035 >>> all_intents_classification_report_dict = intent_report(true_df, pred_df, return_output_as_dict=True) >>> print(all_intents_classification_report_dict) { '_cancel_': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 100}, '_confirm_': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 161}, ... 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1035} } >>> aliased_intents = { 'smalltalk': ['_greeting_', '_repeat_', 'what', 'hmm', '_confirm_', '_cancel_'], 'oos': ['_oos_', '_ood_', 'audio_silent_background_talking', 'audio_silent', 'broken_voice', 'short_utterance', 'audio_noisy'] } >>> grouped_weighted_average_metrics = intent_report(true_df, pred_df, intent_groups=aliased_intents) >>> grouped_weighted_average_metrics precision recall f1-score support group smalltalk 1.0 1.0 1.0 398 oos 1.0 1.0 1.0 274 in_scope 1.0 1.0 1.0 363 >>> grouped_intents_classification_report = intent_report(true_df, pred_df, intent_groups=aliased_intents, breakdown=True) >>> grouped_intents_classification_report { 'smalltalk': precision recall f1-score support _greeting_ 1.000000 1.000000 1.000000 134 _repeat_ 1.000000 1.000000 1.000000 3 what 0.000000 0.000000 0.000000 0 hmm 0.000000 0.000000 0.000000 0 _confirm_ 1.000000 1.000000 1.000000 161 _cancel_ 1.000000 1.000000 1.000000 100 micro avg 1.000000 1.000000 1.000000 398 macro avg 0.666667 0.666667 0.666667 398 weighted avg 1.000000 1.000000 1.000000 398, 'oos': precision recall f1-score support _oos_ 1.000000 1.000000 1.000000 97 _ood_ 0.000000 0.000000 0.000000 0 audio_silent_background_talking 1.000000 1.000000 1.000000 66 audio_silent 1.000000 1.000000 1.000000 32 broken_voice 1.000000 1.000000 1.000000 1 short_utterance 1.000000 1.000000 1.000000 23 audio_noisy 1.000000 1.000000 1.000000 55 micro avg 1.000000 1.000000 1.000000 274 macro avg 0.857143 0.857143 0.857143 274 weighted avg 1.000000 1.000000 1.000000 274, 'in_scope': precision recall f1-score support booking_cancellation 1.0 1.0 1.0 22 booking_status 1.0 1.0 1.0 33 booking_modification 1.0 1.0 1.0 14 new_booking 1.0 1.0 1.0 174 early_checkin 1.0 1.0 1.0 12 cancel_charges 1.0 1.0 1.0 9 request_agent 1.0 1.0 1.0 28 checkin_procedure 1.0 1.0 1.0 29 location_information 1.0 1.0 1.0 14 checkin_time 1.0 1.0 1.0 4 price_enquiry 1.0 1.0 1.0 14 refund_status 1.0 1.0 1.0 8 late_checkout 1.0 1.0 1.0 2 micro avg 1.0 1.0 1.0 363 macro avg 1.0 1.0 1.0 363 weighted avg 1.0 1.0 1.0 363 } ``` ",
    "url": "/eevee/metrics/intents.html",
    "relUrl": "/metrics/intents.html"
  },"5": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": "# Roadmap ... ",
    "url": "/eevee/roadmap.html",
    "relUrl": "/roadmap.html"
  },"6": {
    "doc": "Entities and Slots",
    "title": "Entities and Slots",
    "content": "Refer to [this](https://github.com/skit-ai/onboarding/blob/master/ml/slot-reporting/slot-evaluation-and-reporting.ipynb) document to understand more about slots, entities, and their metrics. # Slots `TODO` # Entities Eevee let's you calculate all the important _turn level metrics_ for various entities. We tag these data points using tog's, an internal tool, region tagging method. | Metric | Description |---------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| False Negative Rate (FNR) | Ratio of turns where we missed out predicting an entity while the utterance had it. | False Positive Rate (FPR) | Ratio of turns where we predicted an entity while the utterance didn't have it. This usually needs attention in normalization by clearly defining what all states are going to be sampled for evaluation. | Mismatch Rate (MMR) | Within entity predictions, the ratio of cases that are differing in value. For example we predicted '3' instead of '2' for a `number` entity. | Here is the list of entities that are supported: | Type | Support remarks |----------------------------+-----------------| `datetime`, `date`, `time` | internally `datetime` (given) is broken down to `date` and `time`, therefore false positives, false negatives, true positives are considered along `date` & `time` and reported outside. | `pattern` | not yet supported | `number` | supported superficially only. `number` and `people` **are not the same**, we still debating on whether to alias all `number` to `people`, vice-versa or not. | ## Data schema either `true-labels.csv` or `pred-labels.csv` should have rows like these: ``` id, entities 1,\"[{\"\"text\"\": \"\"24 अक्टूबर\"\", \"\"type\"\": \"\"date\"\", \"\"score\"\": 0, \"\"value\"\": \"\"2021-10-24T00:00:00.000+05:30\"\"}]\" 2,\"[{\"\"text\"\": \"\"now\"\", \"\"type\"\": \"\"datetime\"\", \"\"score\"\": 0, \"\"value\"\": \"\"2021-10-14T13:35:17.354+05:30\"\"}]\" 3, [] 4,\"[{\"\"text\"\":\"\"21\"\",\"\"type\"\":\"\"number\"\",\"\"score\"\":1,\"\"value\"\":21}]\" 5,\"[{\"\"text\"\":\"\"Mira Road\"\",\"\"type\"\":\"\"location\"\",\"\"score\"\":0,\"\"value\"\":\"\"mira road\"\"}]\" 6,\"[{\"\"text\"\":\"\"पांच से सात\"\",\"\"type\"\":\"\"datetime\"\",\"\"score\"\":0.3,\"\"value\"\":{\"\"from\"\":{\"\"grain\"\":\"\"hour\"\",\"\"value\"\":\"\"2021-10-14T05:00:00.000+05:30\"\"},\"\"to\"\":{\"\"grain\"\":\"\"hour\"\",\"\"value\"\":\"\"2021-10-14T08:00:00.000+05:30\"\"},\"\"type\"\":\"\"interval\"\"}}]\" ``` the `entities` are in `JSON` format. The above ones are just few samples to demonstrate the example. It is important to note that the csv(s) should contain columns called `id` and `entities`, as the `entities` will be merged on the common `id`. Therefore `id` is expected to be unique. We expect `type` and `value` to be never `null`. We also expect `value` is of appropriate python-datatype. Eg: `type` of `date`, we expect `value` to be an ISO string like `\"2021-10-24T00:00:00.000+05:30\"` instead of `24` (an `integer`). Three important things to note: * we require only entity's `type`, `value` for calculating the `entity_report`, the `body` / `text` or any other key is not required as of now. * if no-prediction / no-annotation has been made for that particular entity leave it blank or [], pandas will parse it as `NaN`, accordingly it'll be processed as false negative / false positive. * Right now, we only support only one `value`, meaning we compare truth and prediction only on the first duckling prediction. Implies our comparisons right now for entities looks likes: `[{}]` vs `[{}]`, in future we'd be supporting `[{}, {}]` vs `[{}, {}, {}, {}, {}]` ## Usage ### Command Line For using it on command line, simply call the sub-command `entity` like shown below: ```shell eevee entity ./true-labels.csv ./pred-labels.csv ``` ``` FPR FNR Mismatch Rate Support Negatives Entity date 0.081612 0.086466 0.377715 6199 18588 detail_kind 0.009233 0.840325 0.013018 5292 19495 duration 0.000444 0.750000 0.000000 36 24751 location 0.004438 0.153505 0.854647 3381 21406 number 0.052483 0.636291 0.286652 2513 22274 time 0.046771 0.098007 0.034070 1204 23583 ``` The above numbers are a mix of standard entities like `date`, `time`, `number` etc but also has tagged categorical entities like `location`, `detail_kind` etc. only on categorical entities we can get extra `breakdown` (pass `--breakdown` flag), which will report entity mismatches on their categorical values, example: ``` $ eevee entity data/oyo-hi-datetime-truth-entities.csv data/oyo-hi-datetime-class9-entities.csv --breakdown precision recall f1-score support _ 0.604215 0.849747 0.706250 7321 detail_kind/check_in_date 0.952381 0.909091 0.930233 44 detail_kind/check_out_date 0.906250 0.906250 0.906250 32 detail_kind/city 0.380282 0.551020 0.450000 49 detail_kind/english 0.250000 0.023622 0.043165 127 ........... location/yelagiri 0.000000 0.000000 0.000000 1 location/yelahanka 0.000000 0.000000 0.000000 1 location/zirakpur 0.000000 0.000000 0.000000 3 weighted average (excluding no_entity) 0.148283 0.144563 0.144732 8709 ``` where `_` represents `NaN` vs `NaN` comparisons. this helps with understanding when it comes to misfiring on no-entities. Also the last row being `weighted average (excluding no_entity)` helps in giving overall weighted average metrics on these categorical entities. ### Python module The `entity_report` and `categorical_entity_report` can be imported using ```python from eevee.metrics.entity import entity_report, categorical_entity_report ``` they take true and pred dataframes as input, just like the CLI version. A common usage pattern for ML modeling is to use entity comparison functions from `eevee.ord.entity` module. A demonstration on how to use `date_eq` and `time_eq`: ```python >>> from eevee.ord.entity.datetime import date_eq, time_eq >>> >>> true_date = {'type': 'date', 'value': '2019-04-21T00:00:00+05:30'} >>> pred_date = {'type': 'date', 'value': '2019-04-21T00:00:00+05:30'} >>> >>> date_eq(true_date, pred_date) True >>> true_time = {'type': 'time', 'value': '2019-04-21T00:11:00+05:30'} >>> pred_time = {'type': 'time', 'value': '2019-04-17T00:11:00+05:30'} >>> >>> time_eq(true_time, pred_time) True ``` ",
    "url": "/eevee/metrics/slots.html",
    "relUrl": "/metrics/slots.html"
  }
}
