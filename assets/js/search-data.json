{"0": {
    "doc": "Speech Recognition",
    "title": "Speech Recognition",
    "content": "# Speech Recognition `TODO` | Metric | Description |--------------------------------------+---------------------------------------------------------------| WER | Word Error Rate | Utterance False Positive Rate (uFPR) | Ratio of cases where non speech utterances were transcribed. | Utterance False Negative Rate (uFNR) | Ratio of cases where utterances where transcribed as silence. | ## Data schema `TODO`: Note about the `` situation in transcription. ## Usage ### Command Line Use the sub-command `asr` like shown below: ```shell eevee asr ./data/tagged.transcriptions.csv ./data/predicted.transcriptions.csv ``` ``` Value Support Metric WER 0.571429 6 Utterance FPR 0.500000 2 Utterance FNR 0.250000 4 ``` ### Python module `TODO` ",
    "url": "/eevee/metrics/asr.html",
    "relUrl": "/metrics/asr.html"
  },"1": {
    "doc": "Data Structures",
    "title": "Data Structures",
    "content": "# Data Structures Eevee works with CSV label dataframes with items as per [these definitions](https://github.com/skit-ai/dataframes/blob/master/protos/labels.proto). Since label dataframes have `id` for referring back to the data, we just focus on labels in this tool. These labels can be true labels of various kind or coming from predictions of different models. This page documents a few general notes about the label representation. Specific details are in the pages for different kinds of metrics [here](./metrics). ## Serialization Each row in the label dataframe CSV is of one of the types defined [here](https://github.com/skit-ai/dataframes/blob/master/protos/labels.proto). In cases where the field type are not primitives, we serialize items in JSON. In Python, this looks like the following: ```python import pandas as pd # Assuming each item in `items` is a list of entities rows = [{\"id\": i, \"entities\": json.dumps(it)} for i, it in enumerate(items)] pd.DataFrame(rows).to_csv(\"./predictions.csv\", index=False) ``` The following is how correctly serialized structure looks like in a labels CSV: ``` \"[[{\"\"am_score\"\": -278.4794, \"\"confidence\"\": 0.9739978, \"\"lm_score\"\": 13.827044, \"\"transcript\"\": \"\"no\"\"}]]\" ``` If you skip JSON dumping, tools like pandas might still serialize like following: ``` \"[[{'am_score': -278.4794, 'confidence': 0.9739978, 'lm_score': 13.827044, 'transcript': 'no'}]]\" ``` But this won't be read back in `eevee` and you will get a `JSONDecodeError` ``` JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 4 (char 3) ``` ",
    "url": "/eevee/data-structures.html",
    "relUrl": "/data-structures.html"
  },"2": {
    "doc": "Metrics",
    "title": "Metrics",
    "content": " ",
    "url": "/eevee/metrics/",
    "relUrl": "/metrics/"
  },"3": {
    "doc": "Home",
    "title": "Home",
    "content": "# Eevee ![](https://img.shields.io/github/v/tag/skit-ai/eevee.svg?style=flat-square) ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/skit-ai/eevee/CI?style=flat-square) `eevee` is a set of standard evaluation utilities for problems that we work on at [Skit](https://skit.ai). You can use `eevee` both as a python module or as a CLI tool. It works on data files with label structures from [dataframes](https://github.com/skit-ai/dataframes) that has standard datatype definitions. See `./data` directory for example files. ## Installation For now, you have to install eevee using Github release URLs. The current version can be installed by using the following: ```bash pip install https://github.com/skit-ai/eevee/releases/download/0.5.3/eevee-0.5.3-py3-none-any.whl ``` ## Usage Once installed, the most common usage pattern involves passing a reference and predicted label dataframes and get report either for human viewing, or get a json for further machine consumption. Here is how you use it for intents: ```bash eevee intent ./tagged.intent.csv ./predicted.intent.csv ``` Similarly, for WER report you can do this: ```bash eevee asr ./data/tagged.transcriptions.csv ./data/predicted.transcriptions.csv ``` ``` Value Support Metric WER 0.571429 6 Utterance FPR 0.500000 2 Utterance FNR 0.250000 4 ``` There are a few advanced unexposed metrics related to ASR. Since they are still work in progress, we have kept a few dependencies from there as _extras_. If you need those, you should install the package in development mode and do `poetry install -E asr`. Then follow the scripts in `./scripts`. ",
    "url": "/eevee/",
    "relUrl": "/"
  },"4": {
    "doc": "Intents",
    "title": "Intents",
    "content": "# Intents `TODO` ## Data Schema `TODO` ## Usage ### Command Line Call the sub-command `intent` like shown below: ```shell eevee intent ./true-labels.csv ./pred-labels.csv ``` ### Python module `TODO` ",
    "url": "/eevee/metrics/intents.html",
    "relUrl": "/metrics/intents.html"
  },"5": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": "# Roadmap ... ",
    "url": "/eevee/roadmap.html",
    "relUrl": "/roadmap.html"
  },"6": {
    "doc": "Entities and Slots",
    "title": "Entities and Slots",
    "content": "Refer to [this](https://github.com/skit-ai/onboarding/blob/master/ml/slot-reporting/slot-evaluation-and-reporting.ipynb) document to understand more about slots, entities, and their metrics. # Slots `TODO` # Entities Eevee let's you calculate all the important _turn level metrics_ for various entities. We tag these data points using tog's, an internal tool, region tagging method. | Metric | Description |---------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| False Negative Rate (FNR) | Ratio of turns where we missed out predicting an entity while the utterance had it. | False Positive Rate (FPR) | Ratio of turns where we predicted an entity while the utterance didn't have it. This usually needs attention in normalization by clearly defining what all states are going to be sampled for evaluation. | Mismatch Rate (MMR) | Within entity predictions, the ratio of cases that are differing in value. For example we predicted '3' instead of '2' for a `number` entity. | Here is the list of entities that are supported: | Type | Support remarks |----------------------------+-----------------| `datetime`, `date`, `time` | internally `datetime` (given) is broken down to `date` and `time`, therefore false positives, false negatives, true positives are considered along `date` & `time` and reported outside. | `pattern` | not yet supported | `number` | supported superficially only. `number` and `people` are supported interchangeably at this point. | ## Data schema either `true-labels.csv` or `pred-labels.csv` should have rows like these: ``` id, entities 1, '[{\"type\": \"date\", \"values\": [{\"value\": \"2019-04-21T00:00:00+05:30\", \"type\": \"value\"}]}]' 2, '[{\"text\": \"6th evening\", \"type\": \"time\", \"values\": [{\"type\": \"interval\", \"value\": {\"from\": \"2021-08-06T18:00:00.000-07:00\", \"to\": \"2021-08-07T00:00:00.000-07:00\"}}]}]' 3, 4, '[{\"text\": \"67\", \"type\": \"number\", \"values\": [{\"type\": \"value\", \"value\": 67}]}]' ``` the `entities` are in `JSON` format. exact schema of entity looks like this: for ordinary `value` types: ``` [ { \"type\": \"entity_type\", # date, time, datetime, number, people etc... \"values\": [ { \"value\": \"entity_value\", # \"2019-04-21T00:00:00+05:30\", 42, etc \"type\": \"value\", } ] } ] ``` for `interval` value type: ``` [ { \"type\": \"entity_type\", # date, time, datetime only \"values\": [ { \"value\": {\"from\": \"...\", \"to\": \"...\"}, \"type\": \"interval\", } ] } ] ``` Three important things to note: * we require only entity's `type`, `values` for calculating the `entity_report`, the `body` / `text` or any other key is not required as of now. * if no-prediction / no-annotation has been made for that particular entity leave it blank, pandas will parse it as `NaN`, accordingly it'll be processed as false negative / false positive. * Right now, we only support only one `value`, meaning we compare truth and prediction only on the first duckling prediction. Implies our comparisons right now for entities looks likes: `[{}]` vs `[{}]`, in future we'd be supporting `[{}, {}]` vs `[{}, {}, {}, {}, {}]` ## Usage ### Command Line For using it on command line, simply call the sub-command `entity` like shown below: ```shell eevee entity ./true-labels.csv ./pred-labels.csv ``` ``` FPR FNR Mismatch Rate Support Positives Negatives Entity date 1.0 0.142857 0.0 7 7 1 people 0.0 0.333333 0.0 6 6 0 time 1.0 0.125000 0.0 8 8 3 ``` ### Python module A common usage pattern for ML modeling is to use entity comparison functions from `eevee.ord.entity` module. A demonstration on how to use `date_eq` and `time_eq`: ```python >>> from eevee.ord.entity.datetime import date_eq, time_eq >>> >>> true_date = {'type': 'date', 'values': [{'value': '2019-04-21T00:00:00+05:30', 'type': 'value'}]} >>> pred_date = {'type': 'date', 'values': [{'value': '2019-04-21T00:00:00+05:30', 'type': 'value'}]} >>> date_eq(true_date, pred_date) True >>> true_time = {'type': 'time', 'values': [{'value': '2019-04-21T09:00:00+05:30', 'type': 'value'}]} >>> pred_time = {'type': 'time', 'values': [{'value': '2019-04-21T00:00:00+05:30', 'type': 'value'}]} >>> time_eq(true_time, pred_time) False ``` ",
    "url": "/eevee/metrics/slots.html",
    "relUrl": "/metrics/slots.html"
  }
}
