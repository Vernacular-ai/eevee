{"0": {
    "doc": "Speech Recognition",
    "title": "Speech Recognition",
    "content": "# Speech Recognition `TODO` | Metric | Description |--------------------------------------+---------------------------------------------------------------| WER | Word Error Rate | Utterance False Positive Rate (uFPR) | Ratio of cases where non speech utterances were transcribed. | Utterance False Negative Rate (uFNR) | Ratio of cases where utterances where transcribed as silence. | SER | Sentence Error Rate | Min 3 WER | The minimum Word Error Rate when considering the first three alternatives only | Min WER | The minimum Word Error Rate out of all the alternatives | Short Utterance WER | WER of utterance with ground truth length of 1 or 2 words | Long Utterance WER | WER of utterances with at least 3 words in ground truth | ## Data schema We expect, - tagged.transcriptions.csv to have columns called `id` and `transcription`, where `transcription` can have only one string as value for each row, if not present leave it empty as it is, it'll get parsed as `NaN`. - predicted.transcriptions.csv to have columns called `id` and `utterances`, where **each value** in the `utterances` column looks like this: ``` '[[ {\"confidence\": 0.94847125, \"transcript\": \"iya iya iya iya iya\"}, {\"confidence\": 0.9672866, \"transcript\": \"iya iya iya iya\"}, {\"confidence\": 0.8149829, \"transcript\": \"iya iya iya iya iya iya\"} ]]' ``` as you might have noticed it is expected to be in `JSON` format. each `transcript` represents each alternative from the ASR, and `confidence` represents ASR's confidence for that particular alternative. If no such `utterances` present for that particular `id`, leave it as `'[]'` (`json.dumps` of empty list `[]`) Note: Please remove `transcription` column from predicted.transcriptions.csv (if it exists) before using `eevee`. ## Usage ### Command Line Use the sub-command `asr` like shown below: ```shell eevee asr ./data/tagged.transcriptions.csv ./data/predicted.transcriptions.csv ``` ``` Value Support Metric WER 0.571429 6 Utterance FPR 0.500000 2 Utterance FNR 0.250000 4 SER 0.666667 6 Min 3 WER 0.571429 6 Min WER 0.571429 6 Short Utterance WER 0.000000 1 Long Utterance WER 0.809524 3 ``` For users who want utterance level metrics or edit operations, add the \"--dump\" flag like: ```shell eevee asr ./data/tagged.transcriptions.csv ./data/predicted.transcriptions.csv --dump ``` This will add two csv files - **predicted.transcriptions-dump.csv** : File containing utterance level metrics - **predicted.transcriptions-ops.csv** : File containing dataset level edit operations. The filename is based on the prediction filename given by the user ### Python module ```python >>> import pandas as pd >>> from eevee.metrics.asr import asr_report >>> >>> true_df = pd.read_csv(\"data/tagged.transcriptions.csv\", usecols=[\"id\", \"transcription\"]) >>> pred_df = pd.read_csv(\"data/predicted.transcriptions.csv\", usecols=[\"id\", \"utterances\"]) >>> >>> asr_report(true_df, pred_df) Value Support Metric WER 0.571429 6 Utterance FPR 0.500000 2 Utterance FNR 0.250000 4 SER 0.666667 6 Min 3 WER 0.571429 6 Min WER 0.571429 6 Short Utterance WER 0.000000 1 Long Utterance WER 0.809524 3 ``` ",
    "url": "/eevee/metrics/asr.html",
    "relUrl": "/metrics/asr.html"
  },"1": {
    "doc": "Data Structures",
    "title": "Data Structures",
    "content": "# Data Structures Eevee works with CSV label dataframes with items as per [these definitions](https://github.com/skit-ai/dataframes/blob/master/protos/labels.proto). Since label dataframes have `id` for referring back to the data, we just focus on labels in this tool. These labels can be true labels of various kind or coming from predictions of different models. This page documents a few general notes about the label representation. Specific details are in the pages for different kinds of metrics [here](./metrics). ## Serialization Each row in the label dataframe CSV is of one of the types defined [here](https://github.com/skit-ai/dataframes/blob/master/protos/labels.proto). In cases where the field type are not primitives, we serialize items in JSON. In Python, this looks like the following: ```python import pandas as pd # Assuming each item in `items` is a list of entities rows = [{\"id\": i, \"entities\": json.dumps(it)} for i, it in enumerate(items)] pd.DataFrame(rows).to_csv(\"./predictions.csv\", index=False) ``` The following is how correctly serialized structure looks like in a labels CSV: ``` \"[[{\"\"am_score\"\": -278.4794, \"\"confidence\"\": 0.9739978, \"\"lm_score\"\": 13.827044, \"\"transcript\"\": \"\"no\"\"}]]\" ``` If you skip JSON dumping, tools like pandas might still serialize like following: ``` \"[[{'am_score': -278.4794, 'confidence': 0.9739978, 'lm_score': 13.827044, 'transcript': 'no'}]]\" ``` But this won't be read back in `eevee` and you will get a `JSONDecodeError` ``` JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 4 (char 3) ``` ",
    "url": "/eevee/data-structures.html",
    "relUrl": "/data-structures.html"
  },"2": {
    "doc": "Metrics",
    "title": "Metrics",
    "content": " ",
    "url": "/eevee/metrics/",
    "relUrl": "/metrics/"
  },"3": {
    "doc": "Home",
    "title": "Home",
    "content": "# Eevee ![](https://img.shields.io/github/v/tag/skit-ai/eevee.svg?style=flat-square) ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/skit-ai/eevee/CI?style=flat-square) `eevee` is a set of standard evaluation utilities for problems that we work on at [Skit](https://skit.ai). You can use `eevee` both as a python module or as a CLI tool. It works on data files with label structures from [dataframes](https://github.com/skit-ai/dataframes) that has standard datatype definitions. See `./data` directory for example files. ## Installation For now, you have to install eevee using Github release URLs. The current version can be installed by using the following: ```bash pip install https://github.com/skit-ai/eevee/releases/download/1.2.0/eevee-1.2.0-py3-none-any.whl ``` ## Usage Once installed, the most common usage pattern involves passing a reference and predicted label dataframes and get report either for human viewing, or get a json for further machine consumption. Here is how you use it for intents: ```bash eevee intent ./tagged.intent.csv ./predicted.intent.csv ``` Similarly, for WER report you can do this: ```bash eevee asr ./data/tagged.transcriptions.csv ./data/predicted.transcriptions.csv ``` ``` Value Support Metric WER 0.571429 6 Utterance FPR 0.500000 2 Utterance FNR 0.250000 4 ``` There are a few advanced unexposed metrics related to ASR. Since they are still work in progress, we have kept a few dependencies from there as _extras_. If you need those, you should install the package in development mode and do `poetry install -E asr`. Then follow the scripts in `./scripts`. ",
    "url": "/eevee/",
    "relUrl": "/"
  },"4": {
    "doc": "Intents",
    "title": "Intents",
    "content": "# Intents Eevee let's you calculate all the important _turn level metrics_ (precision, recall, f1) for intents. We tag these data points using tog, an internal tool ## Data Schema We expect the csv(s) to have `id` and `intent` columns. They will be inner-joined on `id`. `id` is expected from the user to be unique. `intent` column should have values whch are of `str` type. ## Usage ### Command Line Call the sub-command `intent` like shown below: ```shell eevee intent ./true-labels.csv ./pred-labels.csv ``` that takes up the csv's merges them on `id` column, to perform [sklearn's classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) on the intents. ### aliasing there is another feature, `alias`-ing ``` eevee intent ./true-labels.csv ./pred-labels.csv --alias-yaml=assets/alias.yaml ``` alias-yaml, helps with situations where there are different intents which are all just the same: intents like: ```yaml - _confirm_browse_ - _confirm_wifi_ - _confirm_power_indicator_ - _confirm_reconfirm_pincode_ - _confirm_next_to_device_ ``` all are just representing the smalltalk intent, `_confirm_`, therefore one could replace them all with `_confirm_`. this is what the `alias.yaml` does. alias-yaml helps replacing intents with what their mother/actual intent you want it to be. this acts as a preprocessing step. example of an `alias.yaml`: ```yaml _confirm_: - _confirm_ - _flickering_ - _confirm_reconfirm_pincode - confirm_new_connection - _confirm_browse_ - _confirm_wifi_ - _confirm_power_indicator_ - _confirm_reconfirm_pincode_ - _confirm_next_to_device_ _cancel_: - _cancel_ - _cancel_lights_steady_ - _cannot_ - _cancel_device_switched_on_ - _cancel_browse_ ``` where `_confirm_` and `_cancel_` replaces all the intnets mentioned below them in the list, in both ground-truth and predictions. ### grouping ``` eevee intent ./true-labels.csv ./pred-labels.csv --groups-yaml=assets/groups.yaml ``` This helps with grouping intents as their respective group-name: In the sample file `groups.yaml` under `assets` directory, we have intents (from the `true-labels.csv` and `pred-labels.csv`) grouped under: * smalltalk_intents * critical_intents * oos_intents (out-of-scope) but you could name or group the intents according to how you wish. The remaining intents which are not part of the groups are grouped as `in_scope` by default. This returns the `weighted_average` of [sklearn's precision_recall_fscore_support](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) Further granular analysis on grouping is also possible. where each group has its own [sklearn's classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) using this: ``` eevee intent ./true-labels.csv ./pred-labels.csv --groups-yaml=assets/groups.yaml --breakdown ``` ### layers (of an intent) ``` eevee intent layers ./true-labels.csv ./pred-labels.csv --layers-yaml=assets/layers.yaml ``` We often need to *break up* intents into sub-intents. The reasons for this range from client demands to (potential) improved performance. But, in the fragile time-space between tagging the new sub-intents in a test set and actually training a model that predicts the new intents, we dont have a way of evaluating performance - the predicted and true labels just dont match up. This occurrence motivates the need for **intent layers**. As convention, the older intent is the name of the layer, and the newer sub-intents are the constituents of that layer. For example, `OOS` was an older intent that we broke up into the newer intents `Acoustic OOS` and `Lexical OOS`. So here, `OOS` is an intent layer, made up of `Acoustic OOS` and `Lexical OOS`. There is a the sample file `layers.yaml` under `assets` directory, which we recommend you use, to set up an intent layer. The current set up only allows evaluating one intent layer at a time, so you might need multiple runs. Further granular analysis on layering is also possible. where each layer has its own [sklearn's classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) using this: ``` eevee intent layers ./true-labels.csv ./pred-labels.csv --layers-yaml=assets/layers.yaml --breakdown ``` ## JSON support All the above mentioned commands use cases, have additional `--json` flag which will be given out in stdout and can be parsed using tools like `jq`. ### Python module ```python >>> import pandas as pd >>> from pprint import pprint >>> from eevee.metrics import intent_report, intent_layers_report :: stanza not found >>> >>> true_df = pd.read_csv(\"data/labels_13_2071.csv\") >>> pred_df = pd.read_csv(\"data/tagged_data_13_2071.csv\") >>> >>> all_intents_classification_report = intent_report(true_df, pred_df) >>> print(all_intents_classification_report) precision recall f1-score support _ 0.00 0.00 0.00 0 _cancel_ 0.82 0.90 0.86 80 _cancel_browse_ 0.00 0.00 0.00 6 ... other_language 0.00 0.00 0.00 1 accuracy 0.37 967 macro avg 0.21 0.19 0.18 967 weighted avg 0.30 0.37 0.33 967 >>> all_intents_classification_report_dict = intent_report(true_df, pred_df, return_output_as_dict=True) >>> all_intents_classification_report { '_': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, '_cancel_': {'precision': 0.8181818181818182, 'recall': 0.9, 'f1-score': 0.8571428571428572, 'support': 80}, '_cancel_browse_': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 6}, '_cancel_internet_connected_': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, ... 'weighted avg': {'precision': 0.3037583323260676, 'recall': 0.37228541882109617, 'f1-score': 0.33185064213224263, 'support': 967}} >>> # aliasing intents >>> aliased_intents = { ... \"_confirm_\": [ ... \"_confirm_\", ... \"_flickering_\", ... \"_confirm_reconfirm_pincode\", ..... ], ... \"_cancel_\": [ ... \"_cancel_\", ... \"_cancel_lights_steady_\", ... \"_cannot_\", ..... ], ..... } >>> aliased_classification_report = intent_report(true_df, pred_df, intent_aliases=aliased_intents) >>> print(aliased_classification_report) precision recall f1-score support _ 0.00 0.00 0.00 0 _cancel_ 0.92 0.87 0.90 94 _confirm_ 0.89 0.92 0.90 372 _greeting_ 0.71 1.00 0.83 5 ... other_language 0.00 0.00 0.00 1 accuracy 0.46 967 macro avg 0.39 0.36 0.35 967 weighted avg 0.46 0.46 0.46 967 >>> # grouping intents >>> grouped_intents = { ... \"oos_intents\": [ ... \"acoustic_oos\", ... \"lexical_oos\" ... ], ... \"smalltalk_intents\": [ ... \"_confirm_\", ... \"_cancel_\", ... \"_repeat_\", ... \"_what_\", ... \"_greeting_\", ... \"request_agent\" ... ] ... } >>> grouped_weighted_average_metrics = intent_report(true_df, pred_df, intent_groups=grouped_intents) >>> print(grouped_weighted_average_metrics) precision recall f1-score support group oos_intents 0.000000 0.000000 0.000000 0 smalltalk_intents 0.723654 0.915119 0.807068 377 in_scope 0.035452 0.025424 0.028195 590 >>> grouped_weighted_average_metrics = intent_report(true_df, pred_df, intent_groups=grouped_intents, breakdown=True) >>> pprint(grouped_weighted_average_metrics) { 'in_scope': precision recall f1-score support _cancel_wifi_ID_connected_ 0.000000 0.000000 0.000000 3 _request_agent_ 1.000000 0.500000 0.666667 6 _confirm_next_to_device_ 0.000000 0.000000 0.000000 28 audio_silent 0.000000 0.000000 0.000000 116 _confirm_switched_on_ 0.000000 0.000000 0.000000 40 _confirm_power_indicator_ 0.000000 0.000000 0.000000 4 inform_name 0.000000 0.000000 0.000000 4 _cancel_internet_connected_ 0.000000 0.000000 0.000000 1 _inform_address_ 1.000000 1.000000 1.000000 4 _cancel_browse_ 0.000000 0.000000 0.000000 6 audio_speech_volume 0.000000 0.000000 0.000000 2 other_language 0.000000 0.000000 0.000000 1 _wait_ 0.000000 0.000000 0.000000 2 _cancel_next_to_device_ 0.000000 0.000000 0.000000 0 background_noise 0.000000 0.000000 0.000000 272 _ 0.000000 0.000000 0.000000 0 _confirm_new_connection_ 0.000000 0.000000 0.000000 2 audio_channel_noise_hold 0.000000 0.000000 0.000000 2 _inform_old_customer_ 1.000000 0.600000 0.750000 5 audio_channel_noise 0.000000 0.000000 0.000000 1 _confirm_wifi_ID_connected_ 0.000000 0.000000 0.000000 5 _cancel_switch_on_device_ 0.000000 0.000000 0.000000 3 internet_not_working 0.000000 0.000000 0.000000 0 _cancel_lights_steady_ 0.000000 0.000000 0.000000 1 background_speech 0.000000 0.000000 0.000000 49 _confirm_browse_ 0.000000 0.000000 0.000000 1 _hathway_plans_ 1.000000 0.500000 0.666667 2 _ood_ 0.000000 0.000000 0.000000 0 _inform_residential_connection_ 1.000000 0.500000 0.666667 2 _oos_ 0.333333 0.400000 0.363636 5 _internet_not_working_ 0.000000 0.000000 0.000000 3 _where_did_you_know_ 0.250000 1.000000 0.400000 1 _inform_name_ 0.000000 0.000000 0.000000 0 audio_speech_unclear 0.000000 0.000000 0.000000 19 micro avg 0.030801 0.025424 0.027855 590 macro avg 0.164216 0.132353 0.132754 590 weighted avg 0.035452 0.025424 0.028195 590, 'oos_intents': precision recall f1-score support acoustic_oos 0.0 0.0 0.0 0 lexical_oos 0.0 0.0 0.0 0 micro avg 0.0 0.0 0.0 0 macro avg 0.0 0.0 0.0 0 weighted avg 0.0 0.0 0.0 0, 'smalltalk_intents': precision recall f1-score support _confirm_ 0.697917 0.917808 0.792899 292 _cancel_ 0.818182 0.900000 0.857143 80 _repeat_ 0.000000 0.000000 0.000000 0 _what_ 0.000000 0.000000 0.000000 0 _greeting_ 0.714286 1.000000 0.833333 5 request_agent 0.000000 0.000000 0.000000 0 micro avg 0.718750 0.915119 0.805134 377 macro avg 0.371731 0.469635 0.413896 377 weighted avg 0.723654 0.915119 0.807068 377 } >>> intent_layers = { ... 'intent_x': { ... 'acoustic_oos': [ 'audio_channel_noise', 'audio_channel_noise_hold', 'audio_speech_unclear','audio_speech_volume', 'audio_silent', 'background_noise', 'background_speech', 'other_language', '_' ], ... 'lexical_oos': ['partial', 'ood', '_oos_'] ... }, ... 'intent_y': { ... 'oos': ['oos', '_'] ... } ... } >>> >>> intent_layers_report(true_df, pred_df, intent_layers=intent_layers) precision recall f1-score support layer layer-acoustic_oos 0.934685 0.898268 0.916115 462 layer-lexical_oos 0.000000 0.000000 0.000000 5 layer-oos 0.934685 0.888651 0.911087 467 >>> out = intent_layers_report(true_df, pred_df, intent_layers=intent_layers, breakdown=True) >>> pprint(out) { 'layer-acoustic_oos': precision recall f1-score support acoustic_oos 0.934685 0.898268 0.916115 462 micro avg 0.934685 0.898268 0.916115 462 macro avg 0.934685 0.898268 0.916115 462 weighted avg 0.934685 0.898268 0.916115 462, 'layer-lexical_oos': precision recall f1-score support lexical_oos 0.0 0.0 0.0 5 micro avg 0.0 0.0 0.0 5 macro avg 0.0 0.0 0.0 5 weighted avg 0.0 0.0 0.0 5, 'layer-oos': precision recall f1-score support oos 0.934685 0.888651 0.911087 467 micro avg 0.934685 0.888651 0.911087 467 macro avg 0.934685 0.888651 0.911087 467 weighted avg 0.934685 0.888651 0.911087 467 } ``` ",
    "url": "/eevee/metrics/intents.html",
    "relUrl": "/metrics/intents.html"
  },"5": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": "# Roadmap ... ",
    "url": "/eevee/roadmap.html",
    "relUrl": "/roadmap.html"
  },"6": {
    "doc": "Entities and Slots",
    "title": "Entities and Slots",
    "content": "Refer to [this](https://github.com/skit-ai/onboarding/blob/master/ml/slots-and-entities/slots-and-entities.ipynb) document to understand more about slots, entities, and their metrics. # Slots `TODO` # Entities Eevee let's you calculate all the important _turn level metrics_ for various entities. We tag these data points using tog's, an internal tool, region tagging method. | Metric | Description |---------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| False Negative Rate (FNR) | Ratio of turns where we missed out predicting an entity while the utterance had it. | False Positive Rate (FPR) | Ratio of turns where we predicted an entity while the utterance didn't have it. This usually needs attention in normalization by clearly defining what all states are going to be sampled for evaluation. | Mismatch Rate (MMR) | Within entity predictions, the ratio of cases that are differing in value. For example we predicted '3' instead of '2' for a `number` entity. | Here is the list of entities that are supported: | Type | Support remarks |----------------------------+-----------------| `datetime`, `date`, `time` | internally `datetime` (given) is broken down to `date` and `time`, therefore false positives, false negatives, true positives are considered along `date` & `time` and reported outside. | `pattern` | not yet supported | `number` | supported superficially only. `number` and `people` **are not the same**, we still debating on whether to alias all `number` to `people`, vice-versa or not. | ## Data schema either `true-labels.csv` or `pred-labels.csv` should have rows like these: ``` id, entities 1,\"[{\"\"text\"\": \"\"24 अक्टूबर\"\", \"\"type\"\": \"\"date\"\", \"\"score\"\": 0, \"\"value\"\": \"\"2021-10-24T00:00:00.000+05:30\"\"}]\" 2,\"[{\"\"text\"\": \"\"now\"\", \"\"type\"\": \"\"datetime\"\", \"\"score\"\": 0, \"\"value\"\": \"\"2021-10-14T13:35:17.354+05:30\"\"}]\" 3, [] 4,\"[{\"\"text\"\":\"\"21\"\",\"\"type\"\":\"\"number\"\",\"\"score\"\":1,\"\"value\"\":21}]\" 5,\"[{\"\"text\"\":\"\"Mira Road\"\",\"\"type\"\":\"\"location\"\",\"\"score\"\":0,\"\"value\"\":\"\"mira road\"\"}]\" 6,\"[{\"\"text\"\":\"\"पांच से सात\"\",\"\"type\"\":\"\"datetime\"\",\"\"score\"\":0.3,\"\"value\"\":{\"\"from\"\":{\"\"grain\"\":\"\"hour\"\",\"\"value\"\":\"\"2021-10-14T05:00:00.000+05:30\"\"},\"\"to\"\":{\"\"grain\"\":\"\"hour\"\",\"\"value\"\":\"\"2021-10-14T08:00:00.000+05:30\"\"},\"\"type\"\":\"\"interval\"\"}}]\" ``` the `entities` are in `JSON` format. The above ones are just few samples to demonstrate the example. It is important to note that the csv(s) should contain columns called `id` and `entities`, as the `entities` will be merged on the common `id`. Therefore `id` is expected to be unique. We expect `type` and `value` to be never `null`. We also expect `value` is of appropriate python-datatype. Eg: `type` of `date`, we expect `value` to be an ISO string like `\"2021-10-24T00:00:00.000+05:30\"` instead of `24` (an `integer`). Three important things to note: * we require only entity's `type`, `value` for calculating the `entity_report`, the `body` / `text` or any other key is not required as of now. * if no-prediction / no-annotation has been made for that particular entity leave it blank or [], pandas will parse it as `NaN`, accordingly it'll be processed as false negative / false positive. * Right now, we only support only one `value`, meaning we compare truth and prediction only on the first duckling prediction. Implies our comparisons right now for entities looks likes: `[{}]` vs `[{}]`, in future we'd be supporting `[{}, {}]` vs `[{}, {}, {}, {}, {}]` ## Usage ### Command Line For using it on command line, simply call the sub-command `entity` like shown below: ```shell eevee entity ./true-labels.csv ./pred-labels.csv ``` ``` FPR FNR Mismatch Rate Support Negatives Entity date 0.081612 0.086466 0.377715 6199 18588 detail_kind 0.009233 0.840325 0.013018 5292 19495 duration 0.000444 0.750000 0.000000 36 24751 location 0.004438 0.153505 0.854647 3381 21406 number 0.052483 0.636291 0.286652 2513 22274 time 0.046771 0.098007 0.034070 1204 23583 ``` The above numbers are a mix of standard entities like `date`, `time`, `number` etc but also has tagged categorical entities like `location`, `detail_kind` etc. only on categorical entities we can get extra `breakdown` (pass `--breakdown` flag), which will report entity mismatches on their categorical values, example: ``` $ eevee entity data/oyo-hi-datetime-truth-entities.csv data/oyo-hi-datetime-class9-entities.csv --breakdown precision recall f1-score support _ 0.604215 0.849747 0.706250 7321 detail_kind/check_in_date 0.952381 0.909091 0.930233 44 detail_kind/check_out_date 0.906250 0.906250 0.906250 32 detail_kind/city 0.380282 0.551020 0.450000 49 detail_kind/english 0.250000 0.023622 0.043165 127 ........... location/yelagiri 0.000000 0.000000 0.000000 1 location/yelahanka 0.000000 0.000000 0.000000 1 location/zirakpur 0.000000 0.000000 0.000000 3 weighted average (excluding no_entity) 0.148283 0.144563 0.144732 8709 ``` where `_` represents `NaN` vs `NaN` comparisons. this helps with understanding when it comes to misfiring on no-entities. Also the last row being `weighted average (excluding no_entity)` helps in giving overall weighted average metrics on these categorical entities. ### Python module The `entity_report` and `categorical_entity_report` can be imported using ```python from eevee.metrics.entity import entity_report, categorical_entity_report ``` they take true and pred dataframes as input, just like the CLI version. A common usage pattern for ML modeling is to use entity comparison functions from `eevee.ord.entity` module. A demonstration on how to use `date_eq` and `time_eq`: ```python >>> from eevee.ord.entity.datetime import date_eq, time_eq >>> >>> true_date = {'type': 'date', 'value': '2019-04-21T00:00:00+05:30'} >>> pred_date = {'type': 'date', 'value': '2019-04-21T00:00:00+05:30'} >>> >>> date_eq(true_date, pred_date) True >>> true_time = {'type': 'time', 'value': '2019-04-21T00:11:00+05:30'} >>> pred_time = {'type': 'time', 'value': '2019-04-17T00:11:00+05:30'} >>> >>> time_eq(true_time, pred_time) True ``` ",
    "url": "/eevee/metrics/slots.html",
    "relUrl": "/metrics/slots.html"
  }
}
